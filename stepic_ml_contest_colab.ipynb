{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stepic_ML_competetion_2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dm--p0TUSLM0",
        "6ivfntGRVx-j"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Froztgal/Stepic_ML_Contest/blob/main/stepic_ml_contest_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm--p0TUSLM0"
      },
      "source": [
        "# Описание данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDYAg-zqRIt8"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "[events_train.csv](https://stepik.org/media/attachments/course/4852/event_data_train.zip) - данные о действиях, которые совершают студенты со стэпами\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "* step_id - id стэпа\n",
        "* user_id - анонимизированный id юзера\n",
        "* timestamp - время наступления события в формате unix date\n",
        "* action - событие, возможные значения: \n",
        "  * discovered - пользователь перешел на стэп\n",
        "  * viewed - просмотр шага,\n",
        "  * started_attempt - начало попытки решить шаг, ранее нужно было явно нажать на кнопку - начать решение, перед тем как приступить к решению практического шага\n",
        "  * passed - удачное решение практического шага\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "[submissions_train.csv](https://stepik.org/media/attachments/course/4852/submissions_data_train.zip) - данные о времени и статусах сабмитов к практическим заданиям\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "* step_id - id стэпа\n",
        "* timestamp - время отправки решения в формате unix date\n",
        "* submission_status - статус решения\n",
        "* user_id - анонимизированный id юзера"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ivfntGRVx-j"
      },
      "source": [
        "---\n",
        "\n",
        "# Imports and settings\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlLYrTQkV2na"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from graphviz import Source\n",
        "from IPython.display import SVG\n",
        "from IPython.display import HTML\n",
        "from IPython.display import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLX8U5euAwCT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd863cfc-fe03-4b8b-e877-1bbef2a498bc"
      },
      "source": [
        "%matplotlib inline\n",
        "sns.set(rc={\"figure.figsize\": (20, 10)})\n",
        "sns.set(font_scale=1.5)\n",
        "\n",
        "style = \"<style>svg{width:70% !important; height:70% !important;}</style>\"\n",
        "HTML(style)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>svg{width:70% !important; height:70% !important;}</style>"
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4WSmNwdYf7U"
      },
      "source": [
        "---\n",
        "\n",
        "# Functions and methods\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUhIBhwMZi_E"
      },
      "source": [
        "def add_date(dataframe):\n",
        "  \n",
        "  new_df = dataframe\n",
        "  new_df['date_clock'] = pd.to_datetime(new_df['timestamp'], unit='s')\n",
        "  new_df['date'] = new_df['date_clock'].dt.date\n",
        "  \n",
        "  return new_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vzk5OU_hR71B"
      },
      "source": [
        "def get_all_users(events):\n",
        "\n",
        "  users_id = events[\"user_id\"].unique()\n",
        "  users_id.sort()\n",
        "  users_id = pd.DataFrame(users_id, columns=[\"user_id\"])\n",
        "\n",
        "  return users_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnk2Dx8bfBCR"
      },
      "source": [
        "def get_pivot_table(dataframe, columns, index=\"user_id\", values=\"step_id\", aggfunc=\"count\", fill_value=0):\n",
        "  \n",
        "  new_df = dataframe\n",
        "  new_df = new_df.pivot_table(index=index,\n",
        "                              columns=columns,\n",
        "                              values=values,\n",
        "                              aggfunc=aggfunc,\n",
        "                              fill_value=fill_value).reset_index()\n",
        "  \n",
        "  return new_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB4ISj1lPcUk"
      },
      "source": [
        "def get_scores(submissions, users_id):\n",
        "\n",
        "  scores = get_pivot_table(submissions, \"submission_status\")\n",
        "  scores_dataframe = users_id.merge(scores, on='user_id', how='outer')\n",
        "  scores_dataframe = scores_dataframe.fillna(0)\n",
        "\n",
        "  return scores_dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIHuUKKsSBiV"
      },
      "source": [
        "def add_pass_mark(train_dataframe, treshold):\n",
        "  \n",
        "  new_df = train_dataframe\n",
        "  new_df['passed_course'] = new_df.correct > treshold\n",
        "\n",
        "  return new_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvcTCRoQS957"
      },
      "source": [
        "def get_filtering_timestamp(events, treshold):\n",
        "\n",
        "  users_start_time = events.groupby(\"user_id\", as_index=False) \\\n",
        "  .agg({\"timestamp\": \"min\"}) \\\n",
        "  .rename({\"timestamp\": \"first_timestamp\"}, axis=1)\n",
        "  \n",
        "  users_start_time[\"user_learning_time_treshold\"] = \\\n",
        "  users_start_time.user_id.map(str) + \"_\" + \\\n",
        "  (users_start_time.first_timestamp + treshold).map(str)\n",
        "\n",
        "  users_start_time = users_start_time.drop(columns=[\"first_timestamp\"], axis=1)\n",
        "\n",
        "  return users_start_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "GrppBESVgurY"
      },
      "source": [
        "def get_time_features(events):\n",
        "\n",
        "  time_features = events.groupby(\"user_id\", as_index=False) \\\n",
        "  .agg({\"timestamp\": \"min\"}) \\\n",
        "  .rename({\"timestamp\": \"start_timestamp\"}, axis=1)\n",
        "\n",
        "  time_features['date_clock'] = pd.to_datetime(time_features['start_timestamp'], unit='s')\n",
        "  time_features['start_year'] = time_features['date_clock'].dt.year\n",
        "  time_features['start_quarter'] = time_features['date_clock'].dt.quarter\n",
        "  time_features['start_month'] = time_features['date_clock'].dt.month\n",
        "  time_features['start_week'] = time_features['date_clock'].dt.isocalendar().week\n",
        "  time_features['start_day'] = time_features['date_clock'].dt.day\n",
        "  time_features['start_day_of_week'] = time_features['date_clock'].dt.weekday\n",
        "  time_features['start_hour'] = time_features['date_clock'].dt.hour\n",
        "\n",
        "  time_features = time_features.drop(columns=[\"start_timestamp\", \"date_clock\"], axis=1)\n",
        "\n",
        "  return time_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-44F_3RHUzj0"
      },
      "source": [
        "def filter_by_time(dataframe, users_start_time):\n",
        "\n",
        "  new_df = dataframe\n",
        "  new_df[\"user_time\"] = new_df.user_id.map(str) + \"_\" + new_df.timestamp.map(str)\n",
        "  new_df = new_df.merge(users_start_time, on=\"user_id\", how=\"outer\")\n",
        "  new_df = new_df[new_df.user_time <= new_df.user_learning_time_treshold]\n",
        "\n",
        "  return new_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg6YbgpNa1dV"
      },
      "source": [
        "def get_steps_tried(train_submissions):\n",
        "\n",
        "  steps_tried = train_submissions.groupby(\"user_id\", as_index=False). \\\n",
        "   step_id.nunique().rename(columns={\"step_id\": \"steps_tried\"})\n",
        "\n",
        "  return steps_tried"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD-RiOEJhMTv"
      },
      "source": [
        "def get_unique_days(dataframe):\n",
        "\n",
        "  days = dataframe.groupby('user_id').date.nunique().to_frame().reset_index()\n",
        "\n",
        "  return days"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3dYGEgTYjvJ"
      },
      "source": [
        "def get_x_y_train(events, submissions, treshold):\n",
        "  \n",
        "  # Копируем датафреймы\n",
        "  new_events = events\n",
        "  new_submissions = submissions\n",
        "  \n",
        "  # добавляем дату и время из временных меток\n",
        "  new_events = add_date(new_events)\n",
        "  new_submissions = add_date(new_submissions)\n",
        "\n",
        "  # Получаем балы пользователей и помечаем тех кто прошел курс\n",
        "  users_id = get_all_users(new_events)\n",
        "  marked_dataframe = get_scores(new_submissions, users_id)\n",
        "  marked_dataframe = add_pass_mark(marked_dataframe, 40) # 40 балов (курс пройден)\n",
        "\n",
        "  # Получаем время начала курса каждым пользователем и фильтруем записи по порогу времени из условия\n",
        "  users_start_time = get_filtering_timestamp(new_events, treshold)\n",
        "  event_data_train = filter_by_time(new_events, users_start_time)\n",
        "  submission_data_train = filter_by_time(new_submissions, users_start_time)\n",
        "\n",
        "  # Получаем количество попыток пользователей решить задания\n",
        "  steps_tried = get_steps_tried(submission_data_train)\n",
        "\n",
        "  # Получаем количество различных действий пользователей\n",
        "  actions = get_pivot_table(event_data_train, \"action\")\n",
        "  status = get_pivot_table(submission_data_train, \"submission_status\")\n",
        "\n",
        "  # Получаем количесвто уникальных дней пользователей и временные фичи\n",
        "  time_features = get_time_features(new_events)\n",
        "  user_days_events = get_unique_days(event_data_train)\n",
        "  user_days_submissions = get_unique_days(submission_data_train)\n",
        "\n",
        "  # Создаем X_train\n",
        "  X = steps_tried\n",
        "  X = X.merge(status, on=\"user_id\", how=\"outer\")\n",
        "  X = X.merge(actions, on=\"user_id\", how=\"outer\")\n",
        "  X = X.merge(marked_dataframe[[\"user_id\", \"passed_course\"]], on=\"user_id\", how=\"outer\")\n",
        "  X = X.merge(user_days_events, on=\"user_id\", how=\"outer\").rename({\"date\": \"e_days\"}, axis=1)\n",
        "  X = X.merge(user_days_submissions, on=\"user_id\", how=\"outer\").rename({\"date\": \"s_days\"}, axis=1)\n",
        "\n",
        "  # Additional features\n",
        "  X = X.merge(time_features, on=\"user_id\", how=\"outer\")\n",
        "  \n",
        "  # Создаем y_train\n",
        "  y = X.passed_course\n",
        "  y = y.map(int)\n",
        "\n",
        "  # Убираем лишние данные из X_train и заполняем NaN\n",
        "  X = X.fillna(0)\n",
        "  z = X\n",
        "  X = X.drop([\"passed_course\"], axis=1)\n",
        "  X = X.set_index(X.user_id).drop(\"user_id\", axis=1)\n",
        "\n",
        "  return X, y, z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjDF7KmIpDtQ"
      },
      "source": [
        "def get_x_pred(events, submissions):\n",
        "\n",
        "  # Копируем датафреймы\n",
        "  new_events = events\n",
        "  new_submissions = submissions\n",
        "  \n",
        "  # Добавляем дату и время из временных меток\n",
        "  new_events = add_date(new_events)\n",
        "  new_submissions = add_date(new_submissions)\n",
        "\n",
        "  # Получаем количество попыток пользователей решить задания\n",
        "  steps_tried = get_steps_tried(new_submissions)\n",
        "\n",
        "  # Получаем количество различных действий пользователей\n",
        "  actions = get_pivot_table(new_events, \"action\")\n",
        "  status = get_pivot_table(new_submissions, \"submission_status\")\n",
        "\n",
        "  # Получаем количесвто уникальных дней пользователей и временные фичи\n",
        "  time_features = get_time_features(new_events)\n",
        "  user_days_events = get_unique_days(new_events)\n",
        "  user_days_submissions = get_unique_days(new_submissions)\n",
        "\n",
        "  # Создаем X_pred\n",
        "  X = steps_tried\n",
        "  X = X.merge(status, on=\"user_id\", how=\"outer\")\n",
        "  X = X.merge(actions, on=\"user_id\", how=\"outer\")\n",
        "  X = X.merge(user_days_events, on=\"user_id\", how=\"outer\").rename({\"date\": \"e_days\"}, axis=1)\n",
        "  X = X.merge(user_days_submissions, on=\"user_id\", how=\"outer\").rename({\"date\": \"s_days\"}, axis=1)\n",
        "\n",
        "  # Additional features\n",
        "  X = X.merge(time_features, on=\"user_id\", how=\"outer\")\n",
        "\n",
        "  # Убираем лишние данные из X_pred и заполняем NaN\n",
        "  X = X.fillna(0)\n",
        "  X = X.set_index(X.user_id).drop(\"user_id\", axis=1)\n",
        "  X = X.sort_index()\n",
        "\n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7KcyKyvCgurc"
      },
      "source": [
        "def exploratory_data_analys(z, lim=10):\n",
        "  sns.heatmap(z.drop(\"user_id\", axis=1).corr(), annot=True, fmt=\".1f\")\n",
        "  for col in z.columns:\n",
        "    if col not in [\"passed_course\", \"user_id\"]:\n",
        "      plt.figure(col)\n",
        "      if z[col].nunique() > lim:\n",
        "        sns.lineplot(data=z, x=z[col].map(float), y=z.passed_course)\n",
        "      else:\n",
        "        tmp = z.passed_course.map(int)\n",
        "        sns.barplot(data=z, x=z[col].map(float), y=tmp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "xzZ3xDEBgurc"
      },
      "source": [
        "def rfc(X_train, X_test, y_train, y_test, scaler, cv=5):\n",
        "\n",
        "  params = {\n",
        "    \"n_estimators\": range(10, 1000, 10),\n",
        "    \"criterion\": [\"gini\", \"entropy\"],\n",
        "    \"max_depth\": range(5, 100, 5),\n",
        "    \"min_samples_leaf\": range(5, 100, 5),\n",
        "    \"min_samples_split\": range(5, 100, 5),\n",
        "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
        "    \"class_weight\": [\"balanced\", \"balanced_subsample\"],\n",
        "    \"bootstrap\": [True, False]\n",
        "  }\n",
        "\n",
        "  rtc = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
        "  clf = RandomizedSearchCV(rtc, cv=cv, scoring=\"roc_auc\", param_distributions=params, n_jobs=-1)\n",
        "  pipeline = make_pipeline(scaler, clf)\n",
        "  pipeline.fit(X_train, y_train)\n",
        "\n",
        "  train_score = pipeline.score(X_train, y_train)\n",
        "  test_score = pipeline.score(X_test, y_test)\n",
        "\n",
        "  return pipeline, train_score, test_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_P-KhX0Lgurd"
      },
      "source": [
        "def lrc(X_train, X_test, y_train, y_test, scaler, cv=5):\n",
        "\n",
        "  params = {\n",
        "    \"penalty\": [\"l1\", \"l2\", \"elasticnet\", \"none\"],\n",
        "    \"C\": np.linspace(0.1, 10, 100),\n",
        "    \"fit_intercept\": [True, False],\n",
        "    \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n",
        "    \"max_iter\": range(100, 1000, 10),\n",
        "    \"class_weight\": [\"balanced\", None]\n",
        "  }\n",
        "\n",
        "  lrc = LogisticRegression(n_jobs=-1, random_state=42)\n",
        "  clf = RandomizedSearchCV(lrc, cv=cv, scoring=\"roc_auc\", param_distributions=params, n_jobs=-1)\n",
        "  pipeline = make_pipeline(scaler, clf)\n",
        "  pipeline.fit(X_train, y_train)\n",
        "\n",
        "  train_score = pipeline.score(X_train, y_train)\n",
        "  test_score = pipeline.score(X_test, y_test)\n",
        "\n",
        "  return pipeline, train_score, test_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Da_9M-m1gure"
      },
      "source": [
        "def svc(X_train, X_test, y_train, y_test, scaler, cv=5):\n",
        "\n",
        "  params = {\n",
        "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
        "    \"C\": np.linspace(0.1, 10, 100),\n",
        "    \"shrinking\": [True, False],\n",
        "    \"probability\": [True, False],\n",
        "    \"gamma\": [\"scale\", \"auto\"],\n",
        "    \"max_iter\": range(100, 1000, 10),\n",
        "    \"class_weight\": [\"balanced\", None]\n",
        "  }\n",
        "\n",
        "  svc = SVC(random_state=42, probability=True)\n",
        "  clf = RandomizedSearchCV(svc, cv=cv, scoring=\"roc_auc\", param_distributions=params, n_jobs=-1)\n",
        "  pipeline = make_pipeline(scaler, clf)\n",
        "  pipeline.fit(X_train, y_train)\n",
        "\n",
        "  train_score = pipeline.score(X_train, y_train)\n",
        "  test_score = pipeline.score(X_test, y_test)\n",
        "\n",
        "  return pipeline, train_score, test_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "2caNnX1Ugure"
      },
      "source": [
        "def dtc(X_train, X_test, y_train, y_test, scaler, cv=5):\n",
        "\n",
        "  params = {\n",
        "    \"criterion\": [\"gini\", \"entropy\"],\n",
        "    \"splitter\": [\"best\", \"random\"],\n",
        "    \"max_depth\": range(2, 100, 1),\n",
        "    \"min_samples_split\": range(2, 100, 1),\n",
        "    \"min_samples_leaf\": range(2, 100, 1),\n",
        "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
        "    \"class_weight\": [\"balanced\", None]\n",
        "  }\n",
        "\n",
        "  dtc = DecisionTreeClassifier(random_state=42)\n",
        "  clf = RandomizedSearchCV(dtc, cv=cv, scoring=\"roc_auc\", param_distributions=params, n_jobs=-1)\n",
        "  pipeline = make_pipeline(scaler, clf)\n",
        "  pipeline.fit(X_train, y_train)\n",
        "\n",
        "  train_score = pipeline.score(X_train, y_train)\n",
        "  test_score = pipeline.score(X_test, y_test)\n",
        "\n",
        "  return pipeline, train_score, test_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "394ClexZgurf"
      },
      "source": [
        "def nbc(X_train, X_test, y_train, y_test, scaler):\n",
        "\n",
        "  nbc = GaussianNB()\n",
        "  pipeline = make_pipeline(scaler, nbc)\n",
        "  pipeline.fit(X_train, y_train)\n",
        "\n",
        "  train_score = roc_auc_score(y_train, pipeline.predict_proba(X_train)[:,1])\n",
        "  test_score = roc_auc_score(y_test, pipeline.predict_proba(X_test)[:,1])\n",
        "\n",
        "  return pipeline, train_score, test_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "J6SeMLtGgurf"
      },
      "source": [
        "def gbc(X_train, X_test, y_train, y_test, scaler):\n",
        "\n",
        "  gbc = GradientBoostingClassifier()\n",
        "  pipeline = make_pipeline(scaler, gbc)\n",
        "  pipeline.fit(X_train, y_train)\n",
        "\n",
        "  train_score = roc_auc_score(y_train, pipeline.predict_proba(X_train)[:,1])\n",
        "  test_score = roc_auc_score(y_test, pipeline.predict_proba(X_test)[:,1])\n",
        "\n",
        "  return pipeline, train_score, test_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "foxVi-k5gurf"
      },
      "source": [
        "def nnc(X_train, X_test, y_train, y_test, scaler):\n",
        "\n",
        "  mplc = MLPClassifier(hidden_layer_sizes=(16, 6), learning_rate=\"adaptive\", activation=\"tanh\", max_iter=1000)\n",
        "  pipeline = make_pipeline(scaler, mplc)\n",
        "  pipeline.fit(X_train, y_train)\n",
        "\n",
        "  train_score = roc_auc_score(y_train, pipeline.predict_proba(X_train)[:,1])\n",
        "  test_score = roc_auc_score(y_test, pipeline.predict_proba(X_test)[:,1])\n",
        "\n",
        "  return pipeline, train_score, test_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6LFJ-eZWFDg"
      },
      "source": [
        "По условию, мы должны предсказать используя данные за первые два дня."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIqP1uKUmWtu"
      },
      "source": [
        "learning_time_treshold = 2 * 24 * 60 * 60 # 2 days in seconds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhiPoCzASFoy"
      },
      "source": [
        "train_events_data = pd.read_csv(\"https://stepik.org/media/attachments/course/4852/event_data_train.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYmlwyoGWpYD"
      },
      "source": [
        "train_submission_data = pd.read_csv(\"https://stepik.org/media/attachments/course/4852/submissions_data_train.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W2gq24diB02"
      },
      "source": [
        "X, y, z = get_x_y_train(train_events_data, train_submission_data, learning_time_treshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bXAHz43dgurh"
      },
      "source": [
        "# X.sort_index().head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "nQYh1_Awgurh"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "E-BymRLaguri"
      },
      "source": [
        "# exploratory_data_analys(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Tkb3G9Osguri"
      },
      "source": [
        "---\n",
        "\n",
        "# Подготовка данных для предсказания\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ftvW4bNkguri"
      },
      "source": [
        "pred_events_data = pd.read_csv(\"https://stepik.org/media/attachments/course/4852/events_data_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "T_dXqvA3guri"
      },
      "source": [
        "pred_submission_data = pd.read_csv(\"https://stepik.org/media/attachments/course/4852/submission_data_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7F7lW3CWguri"
      },
      "source": [
        "X_pred = get_x_pred(pred_events_data, pred_submission_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "99Ytu1qBgurj"
      },
      "source": [
        "# X_pred.sort_index().head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "LhLIp36ngurj"
      },
      "source": [
        "---\n",
        "\n",
        "# Random Forest\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "r0Rd1rHRgurj"
      },
      "source": [
        "scaler = StandardScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "2qDCS1X4gurj"
      },
      "source": [
        "pipe_rfc, train_score, test_score = rfc(X_train, X_test, y_train, y_test, scaler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "qAtm-GXdgurj"
      },
      "source": [
        "res_df = pd.DataFrame({\"Classifier\": [\"Random Forest\"], \"train_score\": [train_score], \"test_score\": [test_score]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSW2CidbLPXt"
      },
      "source": [
        "## Random Forest Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbvnOJvyIaek"
      },
      "source": [
        "feature_imp = pd.Series(pipe_rfc[1].best_estimator_.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "fi_df = pd.DataFrame(feature_imp, columns=[\"importance\"]).reset_index()\n",
        "fi_df = fi_df.rename(columns={\"index\": \"feature\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtnugTDXIpt2"
      },
      "source": [
        "# sns.barplot(x=feature_imp, y=feature_imp.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "h0uG7K-xgurk"
      },
      "source": [
        "---\n",
        "\n",
        "# Logistic Regression\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "qR4A1jiUgurk",
        "outputId": "3aa70406-e37f-4b41-8891-f4497fafca52"
      },
      "source": [
        "pipe_lrc, train_score, test_score = lrc(X_train, X_test, y_train, y_test, scaler)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\GAL\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.83289833        nan 0.83404764 0.8300534  0.83529894 0.83029967\n",
            "        nan 0.8328284  0.83407168        nan]\n",
            "  warnings.warn(\n",
            "C:\\Users\\GAL\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\"The max_iter was reached which means \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "mtMlVNm8gurl"
      },
      "source": [
        "res_df = res_df.append({\"Classifier\": \"Logistic Regression\",\n",
        "                        \"train_score\": train_score, \"test_score\": test_score}, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "xv7sS58igurl"
      },
      "source": [
        "---\n",
        "\n",
        "# Naive Bayes\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "R2PsnmDKgurl"
      },
      "source": [
        "pipe_nbc, train_score, test_score = nbc(X_train, X_test, y_train, y_test, scaler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "UMqZ2cZ6gurl"
      },
      "source": [
        "res_df = res_df.append({\"Classifier\": \"Naive Bayes\",\n",
        "                        \"train_score\": train_score, \"test_score\": test_score}, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "aC1I7iZvgurm"
      },
      "source": [
        "---\n",
        "\n",
        "# C-Support Vector Classification\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "4R6pX4cNgurm",
        "outputId": "ebc37001-cbc6-482d-a14d-d69ef2e9e597"
      },
      "source": [
        "pipe_svc, train_score, test_score = svc(X_train, X_test, y_train, y_test, scaler)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\GAL\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=880).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_ld5h4_3gurm"
      },
      "source": [
        "res_df = res_df.append({\"Classifier\": \"Support Vector Classification\",\n",
        "                        \"train_score\": train_score, \"test_score\": test_score}, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "y0zoR6pkgurm"
      },
      "source": [
        "---\n",
        "\n",
        "# Decision Tree\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "rgQN0Hz-gurm"
      },
      "source": [
        "pipe_dtc, train_score, test_score = dtc(X_train, X_test, y_train, y_test, scaler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "59mPTcZygurn"
      },
      "source": [
        "res_df = res_df.append({\"Classifier\": \"Decision Tree\",\n",
        "                        \"train_score\": train_score, \"test_score\": test_score}, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "0qnxCw9jgurn"
      },
      "source": [
        "---\n",
        "\n",
        "# Gradient Boosting\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "xpWga22xgurn"
      },
      "source": [
        "pipe_gbc, train_score, test_score = gbc(X_train, X_test, y_train, y_test, scaler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ztM8vaXtgurn"
      },
      "source": [
        "res_df = res_df.append({\"Classifier\": \"Gradient Boosting\",\n",
        "                        \"train_score\": train_score, \"test_score\": test_score}, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "-2X-WtKZgurn"
      },
      "source": [
        "---\n",
        "\n",
        "# Neural Network\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "A6ZHEw2Dguro"
      },
      "source": [
        "pipe_nnc, train_score, test_score = nnc(X_train, X_test, y_train, y_test, scaler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "lb2n2K0Eguro"
      },
      "source": [
        "res_df = res_df.append({\"Classifier\": \"Neural Network\",\n",
        "                        \"train_score\": train_score, \"test_score\": test_score}, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "cUrgqXlAguro"
      },
      "source": [
        "---\n",
        "\n",
        "# Predictions\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Qb9ruWtIguro"
      },
      "source": [
        "pipes = [pipe_gbc, pipe_rfc, pipe_lrc, pipe_nnc, pipe_dtc, pipe_nbc]\n",
        "labels = [\"pipe_gbc\", \"pipe_rfc\", \"pipe_lrc\", \"pipe_nnc\", \"pipe_dtc\", \"pipe_nbc\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0dp77bGCguro"
      },
      "source": [
        "for i in range(len(pipes)):\n",
        "  y_pred = pipes[i].predict_proba(X_pred)\n",
        "  save_df = pd.DataFrame({\"user_id\": X_pred.index, \"is_gone\": y_pred[:, 1]})\n",
        "  save_df.to_csv(labels[i], index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "oW_x9Fbogurp"
      },
      "source": [
        "Stepic testing\n",
        "  * DTC - 0.8188757872707935\n",
        "  * GBC - 0.8859789381103069\n",
        "  * LRC - 0.8768111830197085\n",
        "  * NBC - 0.8699781774801457\n",
        "  * NNC - 0.7761337425604662\n",
        "  * RFC - 0.8663461009091422"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "bv8wdETggurp"
      },
      "source": [
        "Лучший достигнутый результат получен при использовании Gradient Boosting (Top 30)."
      ]
    }
  ]
}